{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "Do convolution with the Sobel kernel. We will give the pixels in the boundaries a value of 0. during the convolution.\n",
    "\n",
    "$\n",
    "\\text{image} =  \\begin{bmatrix} \n",
    "                1 & 0 & 2 & 3 & 1 \\\\ \n",
    "                3 & 2 & 0 & 7 & 0 \\\\\n",
    "                0 & 6 & 1 & 1 & 4 \n",
    "                \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{kernel} = \\begin{bmatrix} \n",
    "                -1 & 0 & 1 \\\\\n",
    "                -2 & 0 & 2 \\\\\n",
    "                -1 & 0 & 1 \n",
    "                \\end{bmatrix}\n",
    "$\n",
    "\n",
    "The convoluted image will then become:\n",
    "\n",
    "$\n",
    "\\text{image} \\circledast \\text{kernel} = \\begin{bmatrix}\n",
    "                                    2 & -2 - 3 + 4 & - 2 + 7 +6 & - 4 + 2 & -7 - 6  \\\\\n",
    "                                    6 + 4 & - 1 - 6 + 1 + 2 & -4 -6 + 1 + 14 + 3 & -2 -1 +4 +1 & -3 -14 -1 \\\\\n",
    "                                    2 + 6 & -3 + 2 & -2 -12 + 7 + 2 & -2 + 8 & -7 -2 \n",
    "                                    \\end{bmatrix} \n",
    "$\n",
    "\n",
    "$\n",
    "= \\begin{bmatrix}\n",
    "    2 & 0 & 7 & 0 & 0 \\\\\n",
    "    7 & 0 & 7 & 2 & 0 \\\\\n",
    "    7 & 0 & 0 & 6 & 0\n",
    "    \\end{bmatrix} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "The max pooling layer works by reducing the resolution and complexity of the convoluted image. This is done by extracting the maximum value from some subset or grid of the convoluted image. For example it takes the max values of a 3x3 subset and collapses it into one pixel in the convoluted layer. Thus a small translation of an object in an image may still result in the same pixel value in the max pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1c)\n",
    "\n",
    "The kernel is of size 5x5, applied with a stride of 1 and with 6 filters. It is only the kernel size and the stride that affects the shape (Height x Width) of the convolutional layer. In order to keep the shape a padding of 2 on all sides are needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1d)\n",
    "\n",
    "The input image are RGB of shape 512x512. After doing a convolution with a kernel of unknown size with stride 1 and no padding, the shape of the convoluted image are 504x504. In order for this to happen the kernel needs to remove 4 pixels from the top, 4 pixels from the bottom, 4 pixels from the right side and 4 pixels from the left side. To acheive this a kernel of size 9x9 needs to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1e)\n",
    "\n",
    "Subsampling is done after the convolutuion layer using neighborhoods of 2x2 and a stride of two. This will result in both the heigh and width of the convoluted layer being halved. Thus the size of the resulting pooled feature maps in the first pooling layer will be 252x252."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## task 1f)\n",
    "\n",
    "Now the pooled feature map are convoluted with a kernel of 3x3, a stride of 1 and no padding. This will result in the hight and width of the image being reduced by 2. Thus the size of the new image will be 250x250."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## task 1g)\n",
    "\n",
    "The number of parameter in a convolutional layer is given by the following formula:\n",
    "\n",
    "$\n",
    "\\text{number of parameters in convolutional layer} = (m * n * d + 1) * k\n",
    "$\n",
    "\n",
    "where $m$ and $n$ is the width and height, respectivly, of the filter. $d$ is the number of filters in the previous layer and $k$ is the number of filters in the current layer.\n",
    "\n",
    "The pool layer got no parameters.\n",
    "\n",
    "The fully connected layer has parameters. In this layer all different nodes are connected to each other neuron thus this layer has the highest number of parameters. The number of parameters will then be\n",
    "\n",
    "$\n",
    "\\text{number of parameters in fully connected layer} = (c * p + 1) * c\n",
    "$\n",
    "\n",
    "where $c$ is the nunber of neurons in the current layer and $p$ is the number of neurons in the previous layer.\n",
    "\n",
    "Using the formulas presented above, the neural network in this task will have the following number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters are: 594186\n"
     ]
    }
   ],
   "source": [
    "number_of_parameters = (5 * 5 * 3 + 1) * 32 + (5 * 5 * 32 + 1) * 64 + (5 * 5 * 3 + 1) * 128 + (64 * 128 + 1) * 64 + (10 * 64 + 1) * 10\n",
    "print(f\"The number of parameters are: {number_of_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "![](plots/task2_plot.png)\n",
    "\n",
    "### Task 2b)\n",
    "Report your final training, validation and test accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "Describe network architectures in table.\n",
    "\n",
    "\n",
    "### emilNet:\n",
    "The network is built up in the following way:\n",
    "- Image augmentation: ColorJitter. Changes the color of images randomly. \n",
    "\n",
    "#### Conv layer 1:\n",
    "- Conv2d: Convolution layer with 3 input channels and 32 filters. Kernel of 5x5, stride of 1 and padding of 2.\n",
    "- Batch normalization\n",
    "- Tanh activation function\n",
    "- Dropout with probability of 0.2.\n",
    "\n",
    "#### Conv layer 2:\n",
    "- Conv2d: Convolution layer with 32 input channels and 32 filters. Kernel of 5x5, stride of 1 and padding of 2.\n",
    "- MaxPool2d: Max pooling with kernel size of 2 and stride of 2.\n",
    "- Batch normalization\n",
    "- ReLU activation function\n",
    "- Dropout with probability of 0.2.\n",
    "\n",
    "#### Conv layer 3:\n",
    "- Conv2d: Convolution layer with 32 input channels and 64 filters. Kernel of 3x3, stride of 1 and padding of 1.\n",
    "- MaxPool2d: Max pooling with kernel size of 2 and stride of 2.\n",
    "- Batch normalization\n",
    "- Tanh activation function\n",
    "- Dropout with probability of 0.2.\n",
    "\n",
    "#### Conv layer 4:\n",
    "- Conv2d: Convolution layer with 64 input channels and 128 filters. Kernel of 3x3, stride of 1 and padding of 1.\n",
    "- MaxPool2d: Max pooling with kernel size of 2 and stride of 2.\n",
    "- Batch normalization\n",
    "- ReLU activation function\n",
    "- Dropout with probability of 0.2.\n",
    "\n",
    "#### Fully connected layer 1:\n",
    "- Linear layer with 128 * 4 * 4 inputs and 64 outputs.\n",
    "- ReLU activation function\n",
    "\n",
    "#### Fully connected layer 2:\n",
    "- Linear layer with 64 inputs and 64 outputs.\n",
    "- Tanh activation function\n",
    "\n",
    "#### Fully connected layer 3:\n",
    "- Linear layer with 64 inputs and 10 outputs.\n",
    "\n",
    "\n",
    "\n",
    "### Task 3b)\n",
    "Include final accuracy scores and plot for two models\n",
    "\n",
    "#### emilNet:\n",
    "![](plots/emilNet_plot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3c)\n",
    "#### emilNet:\n",
    "##### Methods that improved the network:\n",
    "- Color jitter: This is a data augmentation technique were the brightness, contrast and saturation of the images are changed randomly. The goal of this technique is to simulate and make different lightning conditions. This will make the network better at classifying images in different lightning conditions.\n",
    "- Batch normalization: Normalize the inputs to the layers by re-centering and re-scaling the outputs from the last layer. Results in a faster and more stable convergence of the weights. \n",
    "- Dropout: Dropout is introduced to reduce overfitting. This is done by randomly dropping nodes from layers when training. Thus each layer will be trained in a slightly different way for each iteration. We used a dropout probability of 0.2, higher and lower probabilities resulted in worse performance. \n",
    "- Activation function: We experimented with different activation functions. It turned out that a combination of ReLU and Tanh resulted in the best performance.\n",
    "- Kernel size: The kernel size of the convolution filter are bigger in the start. The first two filters are 5x5 and the next two are 3x3.\n",
    "\n",
    "#### Methods that did not improve the network:\n",
    "- Random flipping augmentation: This is an augmentation technique that flips the image vertically, horizontally or both. When using this with the network the performance got much worse with a validation accuracy of approximatley 0.5. The reason for the poor performance is probably that the neural network are too small and thus is not able to handle the new flipped images. It could also be that if we performed flipping on the validation set as well, the network would have performed better. \n",
    "- Dropout with probability of 0.5: Using dropout with probability of 0.5 resulted in a worse performance. This could be because the network is trained for too few iterations and it could be that dropout had improved the performance if the number of iterations were higher.\n",
    "- Tanh activation functions: We tried to change all activation functions from ReLU to Tanh, this resulted in worse performance.\n",
    "\n",
    "\n",
    "\n",
    "### Task 3d)\n",
    "#### Without batch normalization:\n",
    "Note how much slower the training cconverges without batch normalization.\n",
    "![](plots/emilNet_without_batchnorm_plot.png)\n",
    "\n",
    "#### With batch normalization:\n",
    "![](plots/emilNet_plot.png)\n",
    "\n",
    "\n",
    "### Task 3e)\n",
    "Test accurracy: 0.808\n",
    "\n",
    "![](plots/emilNet_plot.png)\n",
    "\n",
    "\n",
    "\n",
    "### Task 3f)\n",
    "For the best model we see some signs of overfitting, but not much. If you look at the training and validation loss they differ more and more towards the end. This may indicate that the model overfits to the training data, but as this not a big difference we would say it is ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "FILL IN ANSWER and plot \n",
    "\n",
    "## Task 4b)\n",
    "Visualize filters \n",
    "\n",
    "\n",
    "## Task 4c)\n",
    "FILL IN ANSWER. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb98e01f42c7f6065b4a5fce52b0e4fdf9a26dfb707d3a35d10b8d32268997a0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python385jvsc74a57bd06f46f7eecf513ccb9723df9b1e51a51990d29cc5250a5d50f9bea070973a2595"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
