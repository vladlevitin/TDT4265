{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)\n",
    "\n",
    "Show that \n",
    "\n",
    "$\n",
    "w_{ji} := w_{ji} - \\alpha \\frac{\\partial C}{\\partial w_{ji}}\n",
    "$\n",
    "\n",
    "can be written as \n",
    "\n",
    "$\n",
    "w_{ji} := w_{ji} - \\alpha \\delta_j x_i\n",
    "$\n",
    "\n",
    "where $\\delta_j = f´(z_j)\\sum_k w_{kj}\\delta_k$.\n",
    "\n",
    "We apply the chain rule to $ \\alpha \\frac{\\partial C}{\\partial w_{ji}} $ and get the following expression:\n",
    "\n",
    "$\n",
    "\\alpha \\frac{\\partial C}{\\partial w_{ij}} = \\alpha \\sum_k \\frac{\\partial C}{\\partial z_k} \\frac{\\partial z_k}{\\partial a_j} \\frac{\\partial a_j}{\\partial z_j}\n",
    "\\frac{\\partial z_j}{\\partial w_{ji}}\n",
    "$ \n",
    "\n",
    "We then take a look at the different factors in the above expression and solve them independently.\n",
    "\n",
    "$\n",
    "\\frac{\\partial C}{\\partial z_k} = \\delta_k\n",
    "$\n",
    "\n",
    "From previous assignment.\n",
    "\n",
    "$\n",
    "\\frac{\\partial z_k}{\\partial a_j} = \\frac{\\partial}{\\partial a_j} \\left (\\sum_{j´} w_{kj´} a_{j´} \\right) = w_{kj}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial a_j}{\\partial z_j} = \\frac{\\partial f(z_j)}{\\partial z_j} = f´(z_j)\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial z_j}{\\partial w_{ji}} = \\frac{\\partial}{\\partial w_{ji}} \\left( \\sum_{i´ = 0}^d w{ji´}x_{i´} \\right) = x_i\n",
    "$\n",
    "\n",
    "By putting all the above calculations together we get the following expression for $ \\alpha \\frac{\\partial C}{\\partial w_{ji}} $:\n",
    "\n",
    "$\n",
    "\\alpha \\frac{\\partial C}{\\partial w_{ji}} = f´(z_j) \\sum_k w_{kj} \\delta_k\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "In order to write the update rule in matrix notataion we will go through the network, from the input layer to the output layer, and note the dimensions of the different matrices and vectors. The network is defined with $I$ as the number of nodes in the input layer, $J$ as the number of nodes in the hidden layer and $K$ as the number of nodes in the output layer.\n",
    "\n",
    "Define size of input vector and weight matrix between input layer and hidden layer:\n",
    "\n",
    "$X: [I \\times 1]$\n",
    "\n",
    "$W_{ji}: [J \\times I]$\n",
    "\n",
    "Feed the input vector through the input layer:\n",
    "\n",
    "$ z_ j = W_{ji} \\cdot X \\quad [J \\times 1]$ \n",
    "\n",
    "$ a_j = f(z_j) \\quad [J \\times 1]$\n",
    "\n",
    "$a_j$ will be the input for the hidden layer. And the weight matrix from the hidden layer to the output layer will be:\n",
    "\n",
    "$ W_{kj}: [K \\times J]$\n",
    "\n",
    "Feed $a_j$ through the hidden layer:\n",
    "\n",
    "$z_k = W_{kj} \\cdot a_j \\quad [K \\times 1]$\n",
    "\n",
    "$a_k = f(z_k) \\quad [K \\times 1]$\n",
    "\n",
    "The update rules for the weights are\n",
    "\n",
    "$W_{ji} := W_{ji} - \\alpha \\delta_1^T x_i \\quad [J \\times I]$\n",
    "\n",
    "$W_{kj} := W_{kj} - \\alpha \\delta_2^T a_j \\quad [K \\times J]$\n",
    "\n",
    "Since the dimensions of the last part of the update rule needs to be the same as for the weight matrices to be updated, the dimensions of $\\delta_1$ and $\\delta_2$ needs to be:\n",
    "\n",
    "$\\delta_1: [J \\times 1]$\n",
    "\n",
    "$\\delta_2: [K \\times 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "\n",
    "The mean and std are calculated as 33.55274553571429 and 78.87550070784701, respectivly. The are saved as variables in the pre_process_images function so that they are the same for training, validation and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "Here you have a plot of the training and validation loss and a plot shwoing the training and validation accuracy. It looks like the model overfits a bit ti the training data given the difference in the performance on the training and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "The network consists of a input layer with 785 neurons, including the bias, a hidden layer of 64 neurons and an output layer of 10 neurons. Thus the number of weights are $785*64+64*10=50880$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](task3_all_plots.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see a plot showing the validation loss and accuracy given the different \"tricks of the trade\". \n",
    "\n",
    "#### Convergence speed:\n",
    "As you can see from the plots the convergence speed of all three imoproved models are much faster than the model from task 2. Especially for the models with imporved sigmoid and momentum there is a great increase. Also, since these models converges much faster, the early stopping kicks in earlier, saves both time and computation.\n",
    "\n",
    "#### Generalization/overfitting:\n",
    "As earlier stated in task 2 the first models looks to overfit quite a lot given the much better performance on the training data compared to the validation data. For all the improved models this does not happen. All the improved models performs very good on the validation data, implying no overfitting.\n",
    "\n",
    "#### Final accuracy/Validation loss:\n",
    "The final accuracy and validation loss is much better in all three imporved models compared to the original one. All three models has a loss that is almost half the one in the original model and 0.01 better validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "Lowering the number of neurons in the hidden layer from 64 to 32 makes the network perform worse. The validation loss increases by 0.05 and the validation accuracy decreases by approximatley 0.01. The reason behind this worsen performance is probably that the model with 32 neurons in the hidden layer does not catch the features of the numbers as good as the model with 64 neurons, and thus do not manage to classify as good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](task4a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "Increasing the number of neurons in the hidden layer from 64 to 128 makes the model perform better. This is probably beacuse the model with the higher number of neurons manges to catch more of the features seperating the different digits, and thus manages to classify better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](task4b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "The number of parameters in the initial network was 50880. To get approximatley as many neurons we use [60, 60, 10] neurons in each layer giving $785*60+60*60+60*10=50770$ parameters.\n",
    "\n",
    "Below you can see the loss of training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](task4d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network with more layers, but approximatley as many parameters, performs alomst exactly as good as the original from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](task4e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new model with more layers performs worse than the original model. It could be that the training stops to early and that the model could improve even more given more iterations. Also the loss on the training data looks very noisy and that it is struggeling to find a minima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
