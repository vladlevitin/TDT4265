{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)\n",
    "\n",
    "Show that \n",
    "\n",
    "$\n",
    "w_{ji} := w_{ji} - \\alpha \\frac{\\partial C}{\\partial w_{ji}}\n",
    "$\n",
    "\n",
    "can be written as \n",
    "\n",
    "$\n",
    "w_{ji} := w_{ji} - \\alpha \\delta_j x_i\n",
    "$\n",
    "\n",
    "where $\\delta_j = f´(z_j)\\sum_k w_{kj}\\delta_k$.\n",
    "\n",
    "We apply the chain rule to $ \\alpha \\frac{\\partial C}{\\partial w_{ji}} $ and get the following expression:\n",
    "\n",
    "$\n",
    "\\alpha \\frac{\\partial C}{\\partial w_{ij}} = \\alpha \\sum_k \\frac{\\partial C}{\\partial z_k} \\frac{\\partial z_k}{\\partial a_j} \\frac{\\partial a_j}{\\partial z_j}\n",
    "\\frac{\\partial z_j}{\\partial w_{ji}}\n",
    "$ \n",
    "\n",
    "We then take a look at the different factors in the above expression and solve them independently.\n",
    "\n",
    "$\n",
    "\\frac{\\partial C}{\\partial z_k} = \\delta_k\n",
    "$\n",
    "\n",
    "From previous assignment.\n",
    "\n",
    "$\n",
    "\\frac{\\partial z_k}{\\partial a_j} = \\frac{\\partial}{\\partial a_j} \\left (\\sum_{j´} w_{kj´} a_{j´} \\right) = w_{kj}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial a_j}{\\partial z_j} = \\frac{\\partial f(z_j)}{\\partial z_j} = f´(z_j)\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial z_j}{\\partial w_{ji}} = \\frac{\\partial}{\\partial w_{ji}} \\left( \\sum_{i´ = 0}^d w{ji´}x_{i´} \\right) = x_i\n",
    "$\n",
    "\n",
    "By putting all the above calculations together we get the following expression for $ \\alpha \\frac{\\partial C}{\\partial w_{ji}} $:\n",
    "\n",
    "$\n",
    "\\alpha \\frac{\\partial C}{\\partial w_{ji}} = f´(z_j) \\sum_k w_{kj} \\delta_k\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "For this task I have provided the dimensions of the appropriate matrices (mostly for my own sake) to see that the dimensions add up.\n",
    "\n",
    "I choose the following annotation for the weights:\n",
    "\n",
    "$W^j = $ all weights from input->hidden layer. $dim(W^j)=J\\times I$.\n",
    "\n",
    "$A^j = $ all activations from hidden layer. $dim(A^j) = J \\times 1$\n",
    "\n",
    "$W^k = $ all weights from hidden->output layer. $dim(W^k) = K \\times J$\n",
    "\n",
    "$X = $ all inputs. $dim(X)=I\\times 1$\n",
    "\n",
    "$dim(z_k)=K\\times1$\n",
    "\n",
    "### Updating the first layer\n",
    "\n",
    "We have $\\frac{\\partial C}{\\partial w_{kj}}=\\delta_k a_j$ which can be rewritten for the whole layer as as $\\frac{\\partial C}{\\partial W^k}=\\delta_k (A^j)^\\top$\n",
    "\n",
    "A sanity check:\n",
    "\n",
    "The multiplication dimensions then results in: $dim\\left(\\delta_k (A^j)^\\top\\right) = (K \\times 1)(1 \\times J) = (K \\times J) = dim(W^k)$ \n",
    "\n",
    "### Updating the hidden layer\n",
    "\n",
    "\n",
    "$dim(z_j) = dim(w_{ji}^\\top x_i) = (J \\times I) (I \\times 1) = J \\times 1$\n",
    "\n",
    "The update rule for the hidden layer is when we apply $\\frac{\\partial C}{\\partial w_{ji}}$. We know that $\\delta_j x_i = \\frac{\\partial C}{\\partial w_{ji}}$ and $\\delta_j = f'(z_j)\\sum_k w_{kj}\\delta_k$. To vectorize this sum, we can transpose the weight matrix:\n",
    "\\begin{equation}\n",
    "\\delta_j = f'(z_j)((W^k)^\\top \\delta_k)\n",
    "\\end{equation}\n",
    "\n",
    "The vectorized update rule then is $f'(z_j)\\circ\\left(\\left(W^k)^\\top \\delta_k\\right)\\right)X^\\top$\n",
    "\n",
    "here, $\\circ$ is the hadamard product (elementwise multiplication).\n",
    "\n",
    "A sanity check: $dim(\\delta_j) = (J\\times K)(K \\times 1) = J\\times 1$, and $dim(x) = I\\times 1$\n",
    "\n",
    "So, $dim(\\delta_j X^\\top) = (J\\times 1)(1 \\times I) = J \\times I = dim(W^j)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "\n",
    "The mean and std are calculated as 33.55274553571429 and 78.87550070784701, respectivly. The are saved as variables in the pre_process_images function so that they are the same for training, validation and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "Here, I think that the validation loss- does not converge so well because we are stopping at a local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Because of the bias trick, the calculation for the weights is a bit different than it traditionally would be.\n",
    "The first layer has 785 inputs and 64 outputs. The second layer has 64 inputs and 10 outputs. There are no biases. The total number of weights is then $785*64+64*10=50880$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please bear in mind that the implementations are cumulative, so *use momentum* also uses improved sigmoid and improved weight init.\n",
    "![](task3_all_plots.png)\n",
    "\n",
    "| Technique added      |  Final Validation Loss |  Final Validation Accuracy | Early stopping epoch |\n",
    "|----------------------|------------------------|----------------------------|----------------------|\n",
    "| Task 2 model         |               0.295088 |                    0.98342 |                   47 |\n",
    "| Improved weight init |               0.14795  |                    0.99172 |                   45 |\n",
    "| Improved sigmoid     |               0.185989 |                    0.99052 |                   15 |\n",
    "| Use momentum         |               0.183908 |                    0.99146 |                   17 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved weight\n",
    "| Feature | Comment |\n",
    "|---------:|:---------|\n",
    "|Convergence Speed | Converges a lot faster|\n",
    "|Generalization/Overfitting | Can't see any overfitting, but it improves very slowly for a lot of the training |\n",
    "| Final Accuracy/Validation loss | Is a lot better than Task 2. The final validation loss is around half of the previous one |\n",
    "\n",
    "## Improved sigmoid\n",
    "| Feature | Comment |\n",
    "|---------:|:---------|\n",
    "|Convergence Speed | Converges even faster, and we also have way earlier stopping which saves us a lot of computations.|\n",
    "|Generalization/Overfitting | The validation loss starts increasing, but validation accuracy improves. According to [this stackexchange thread](https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy), it means that the network is actually less confident when it does its guess. So, while it might been 70% sure about a given number earlier, it is now only 55% sure. The outcome/guess is still the same, but the validation loss is larger. I don't think this means that it's overfit. |\n",
    "| Final Accuracy/Validation loss | Is worse than the previous one (improved weight), but perhaps this is a tradeoff between how fast the neural network stops. |\n",
    "\n",
    "## Use momentum\n",
    "| Feature | Comment |\n",
    "|---------:|:---------|\n",
    "|Convergence Speed | Seems to converge slower than `improved sigmoid`, but perhaps this is because of the reduced learning rate.|\n",
    "|Generalization/Overfitting | Displays the same phenomenon as in `improved sigmoid`. I don't think this means that it's overfit, though. |\n",
    "| Final Accuracy/Validation loss | Has improved from `improved sigmoid`, but still isn't better than just using `improved weight`. It's interesting to see that the validation loss becomes a lot less jittery towards the end, as seen on the red line on the left part of the figure. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "It seems that if we have too few neurons, overfitting starts occuring even though we haven't reached a super good accuracy.\n",
    "\n",
    "![](task4a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "Having too many neutons seems to cause the network to take longer to stop learning. A somewhat higher validation accuracy was achieved but at the cost of a longer training period.\n",
    "\n",
    "![](task4b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "From task 2d, the number of parameters was found to be 50880.\n",
    "\n",
    "I chose to go with `neurons_per_layer = [60,60,10]` This means that we have $785*60+60*60+60*10=50770$ weights.\n",
    "\n",
    "![](task4d.png)\n",
    "\n",
    "The models seem to be performing pretty similarly, but the `[60, 60, 10]` network takes a bit longer to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "![](task4e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to achieve a comparable accuracy on the validation set, but it spends a very long time converging. I think this comes from the fact that the error keeps approving very slightly, so early stopping does not kick in. Having a lot of layers seems to mean that we can achieve a marginal increase in accuracy, but at the cost of a greatly increaased training period."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
