{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "\\begin{equation}\n",
    "w_{ji}:=w_{ji}-\\alpha\\frac{\\partial C}{\\partial w_{ji}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w_{j i}}=\\frac{\\partial C}{\\partial z_{j}} \\cdot \\frac{\\partial z_{j}}{\\partial w_{j i}}\n",
    "\\end{equation}\n",
    "\n",
    "By substituting\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial z_j} = \\delta_j \\quad \\quad \\frac{\\partial z_j}{\\partial w_{ji}}= x_i\n",
    "\\end{equation}\n",
    "\n",
    "we get:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w_{ji}}=\\delta_j x_i\n",
    "\\end{equation}\n",
    "\n",
    "**Next**, we have\n",
    "\\begin{equation}\n",
    "\\delta_{j}=\\frac{\\partial C}{\\partial z_{j}}=\\frac{\\partial C}{\\partial a_{j}} \\cdot \\frac{\\partial a_{j}}{\\partial z_{j}}=\\frac{\\partial C}{\\partial a_{j}} \\cdot f^{\\prime}\\left(z_{j}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Now we have to find $\\frac{\\partial C}{\\partial a_{j}}$:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial a_{j}}=\\left(\\sum_{k} \\frac{\\partial C}{\\partial z_{k}} \\frac{\\partial z_{k}}{\\partial a_{j}}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "We know that $\\frac{\\partial C}{\\partial z_k}=\\delta_k$. For $\\frac{\\partial z_k}{\\partial a_j}$ we have:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial z_k}{\\partial a_j}&=\\frac{\\partial}{\\partial a_j}\\left(\\sum_j w_{kj}a_j\\right)\\\\\n",
    "&= w_{kj}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Putting this all together yields:\n",
    "\\begin{equation}\n",
    "    \\delta_j = \\frac{\\partial C}{\\partial z_j} = \\frac{\\partial C}{\\partial a_j} f'(z_k) = f'(z_k)\\cdot\\sum_k w_{kj}\\delta_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "For this task I have provided the dimensions of the appropriate matrices (mostly for my own sake) to see that the dimensions add up.\n",
    "\n",
    "I choose the following annotation for the weights:\n",
    "\n",
    "$W^j = $ all weights from input->hidden layer. $dim(W^j)=J\\times I$.\n",
    "\n",
    "$A^j = $ all activations from hidden layer. $dim(A^j) = J \\times 1$\n",
    "\n",
    "$W^k = $ all weights from hidden->output layer. $dim(W^k) = K \\times J$\n",
    "\n",
    "$X = $ all inputs. $dim(X)=I\\times 1$\n",
    "\n",
    "$dim(z_k)=K\\times1$\n",
    "\n",
    "### Updating the first layer\n",
    "\n",
    "We have $\\frac{\\partial C}{\\partial w_{kj}}=\\delta_k a_j$ which can be rewritten for the whole layer as as $\\frac{\\partial C}{\\partial W^k}=\\delta_k (A^j)^\\top$\n",
    "\n",
    "A sanity check:\n",
    "\n",
    "The multiplication dimensions then results in: $dim\\left(\\delta_k (A^j)^\\top\\right) = (K \\times 1)(1 \\times J) = (K \\times J) = dim(W^k)$ \n",
    "\n",
    "### Updating the hidden layer\n",
    "\n",
    "\n",
    "$dim(z_j) = dim(w_{ji}^\\top x_i) = (J \\times I) (I \\times 1) = J \\times 1$\n",
    "\n",
    "The update rule for the hidden layer is when we apply $\\frac{\\partial C}{\\partial w_{ji}}$. We know that $\\delta_j x_i = \\frac{\\partial C}{\\partial w_{ji}}$ and $\\delta_j = f'(z_j)\\sum_k w_{kj}\\delta_k$. To vectorize this sum, we can transpose the weight matrix:\n",
    "\\begin{equation}\n",
    "\\delta_j = f'(z_j)((W^k)^\\top \\delta_k)\n",
    "\\end{equation}\n",
    "\n",
    "The vectorized update rule then is $f'(z_j)\\circ\\left(\\left(W^k)^\\top \\delta_k\\right)\\right)X^\\top$\n",
    "\n",
    "here, $\\circ$ is the hadamard product (elementwise multiplication).\n",
    "\n",
    "A sanity check: $dim(\\delta_j) = (J\\times K)(K \\times 1) = J\\times 1$, and $dim(x) = I\\times 1$\n",
    "\n",
    "So, $dim(\\delta_j X^\\top) = (J\\times 1)(1 \\times I) = J \\times I = dim(W^j)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "I found $X_{mean}=33.55274553571429$ and $X_{std} = 78.87550070784701$. I set these values as parameters with a default value in the pre-processing function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "Here, I think that the validation loss does not converge so well because we are stopping at a local minima.\n",
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "This is a fully conneted neural network. The first layer has 784 inputs and 64 outputs. The second layer has 64 inputs and 10 outputs. The total number of weights is then $784*64+64*10 = 50816$. The number of biases is equal to the number of neurons in each layer, that is: $64+10=74$. The total number of parameters is then $50816+74=50890$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please bear in mind that the implementations are cumulative, so *use momentum* also uses improved sigmoid and improved weight init.\n",
    "![](task3_all_plots.png)\n",
    "\n",
    "| Technique added | Final Validation Loss | Final Validation Accuracy | Early stop epoch |\n",
    "|---------------|---------------|-------------------|--|\n",
    "| Task 2 model | 0.29508757976789135 | 0.98342 | 47 |\n",
    "| Improved weight init | 0.14795016946804032 | 0.99172 | 45 |\n",
    "| Improved sigmoid | 0.22701928787856604 | 0.98892 | 18 |\n",
    "| Use momentum | 0.2402569264405746 | 0.99054 | 22 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved weight\n",
    "| Feature | Comment |\n",
    "|---------|---------|\n",
    "|Convergence Speed | Converges a lot faster|\n",
    "|Generalization/Overfitting | Can't see any overfitting, but it improves very slowly for a lot of the training |\n",
    "| Final Accuracy/Validation loss | Is a lot better than Task 2. The final validation loss is around half of the previous one |\n",
    "\n",
    "## Improved sigmoid\n",
    "| Feature | Comment |\n",
    "|---------|---------|\n",
    "|Convergence Speed | Converges even faster, and we also have way earlier stopping which saves us a lot of computations.|\n",
    "|Generalization/Overfitting | No overfitting here either, and the learning stops quite early which means that the model starts stagnating pretty quickly |\n",
    "| Final Accuracy/Validation loss | Is worse than the previous one (improved weight), but perhaps this is a tradeoff between how fast the neural network stops. |\n",
    "\n",
    "## Use momentum\n",
    "| Feature | Comment |\n",
    "|---------|---------|\n",
    "|Convergence Speed | Seems to converge slower than `improved sigmoid`, but perhaps this is because of the reduced learning rate.|\n",
    "|Generalization/Overfitting | Still no overfitting. |\n",
    "| Final Accuracy/Validation loss | Has improved from `improved sigmoid`, but still isn't better than just using `improved weight`. It's interesting to see that the validation loss becomes a lot less jittery towards the end, as seen on the red line on the left part of the figure. |\n",
    "- Convergence speed:\n",
    "- **Generalization/overfitting:\n",
    "- **Final accuracy/validation loss:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "FILL IN ANSWER. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "FILL IN ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "FILL IN ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "FILL IN ANSWER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
